# LOGISTIC REGRESSION ANALYSIS

##This exercise focuses on exploring the data and performing and interpreting logistic regression analysis

In this exercise, we explore the use of logistic regression analysis on student achievement data in secondary education in two Portuguese schools. The data attribute include the student grades, demographic, social and school related features. The data was collected by using school reports and questionnaires. 

Purpose of the analysis is to study the relationships between high/low alcohol consumption and students attributes. The _joined dataset_ used in the analysis exercise combines the two student alcohol consumption data sets. The following adjustment have been made;  

* 'alc_use is the average of 'Dalc' and 'Walc'  
* 'high_use' is true if 'alc_use' is higher than 2 and FALSE otherwise  

### 1. Load and describe the student alcohol consumption data 

```{r}
#set the work directory
setwd("C:/LocalData/ocholla/IODS-project")
#Load the data
alc<- read.csv("Data/pormath.csv", header = TRUE, stringsAsFactors = TRUE)
#print column names
colnames(alc)
#data structure
str(alc)
```

The dataset contains 370 observation of 51 variables, the variables are a mixture of integers, factors and logical data types

### 2. Choose 4 interesting variables in the data and for each of them , present your personal hypothesis about their relationship with alcohol consumption.

Hypothesis on the four variables  

1. sex- male students consume higher alcohol compared to female students
2. studytime- low study time among the students increase the chances for high use of alcohol
3. famrel- Good quality family relationship reduces alcohol abuse.
4. goout- students who have a high tendency of going out with friends are prone to peer pressure which can easily lead to high alcohol consumption

 check if one of the chosen variables is a factor (refer to the link)

```{r}

```

## Numerically and graphically explore the distribution of your chosen variables and their relationships with alcohol consumption (use crosstabulation, barplots and box plots). comment on your findings and compare the results of ypour previously stated hypotheses. 


```{r}
library(ggplot2)
g1<- ggplot(data= alc, aes(x = high_use, fill= sex))
#define the plot as a bar plot and draw it
g1+ geom_bar()+ ggtitle("Distribution by parents cohabitation status in alcohol consumption")
```

Students from househilds where parents are living together indulge in high alcohol
consumption compared to households with parents living separately

```{r}
#*summary statistics by address and final grades 
library(dplyr)
library(ggplot2)

#produce summary statistics by group
alc%>% group_by(sex, high_use)%>% summarise(count= n(), familytime=mean(famrel))
```

Approximately 10% more of students based in rural address has high use of alcohol compared 
to the urban address students. In the two addresses, the rural students with high alcohol 
consumption had higher mean grade compared to those with low alcohol consumption. 
This is vice verse in the urban areas where the students with low alcohol consumption recorded the highest mean grade



**failures**
```{r}
#*failures
alc%>% group_by(sex, high_use)%>% summarise(count= n(), going_out=mean(goout))

```

In both parent cohabitation status, students with high use of alcohol had higher average failure times. 
The failure average of students with parents living togther had the highest average failure times

**Does high use of alcohol have a connection to school absence?**
```{r}
#does high use of alcohol have a connection to romantic relationship?

a2<- ggplot(alc, aes(x= high_use, y = studytime, col= sex))
#define the plot as a boxplot and draw it
a2 + geom_boxplot()+ylab("Hours")+ggtitle("Student study hours by alcohol consumption and sex")

```

## Fitting a model
present and interpret a summey of the fitted model.
present and interprest the coefficient of the model as odds ratios and provide confidence intervals for them. interpret the results and compare them to your stated hypothesis

Odd ratio: The ratio of expected "successes" to failures are called the odds
Higher odds corresponds to a higher probability of success, with the value ranging from zero to infinity the ratio of two odds is called the odd ratio or the ratio of successes to failures. Odds higher than 1 mean that X is positively associated with "success"

```{r}
#find the model with glm()
m<- glm(high_use~sex+studytime+famrel+goout, data = alc, family = "binomial")
#print out a summary of the model
summary(m)


```

```{r}
#Print out the coefficients of the model
coef(m)
#compute odds ratios (OR)
OR<- coef(m)%>% exp
#compute confidence intervals (CI)
CI<- confint(m)%>% exp
#Print out te odds ratios with their CI
cbind(OR,CI)
```


using the variables which according to your logistic regression model had a statistical relationship with high/low alcohol consumption, explore the predictive power of your model. provide a 2x2 cross tabulation of predictors versus the actual values and optionally display graphic visualizing both the actual values and the predictors. compute the total proportion of inaccurately classified individuals (= the training error) and coment on all the results. compare the performance of the model with performance achieved by some simple guessing strategy

```{r}
#predict() the probability of high_use
#predict() the probability of high_use
probabilities <- predict(m, type = "response")
#add the predicted probabilities to alc
alc<- mutate(alc, probability= probabilities)
#use the probabilities to make a prediction of high_use
alc<- mutate(alc, prediction= probability>0.5)
#*see the first ten original classes, predicted probabilities, 
#*and class predictions
#*
select(alc, goout, famrel,studytime, sex, high_use,probability, prediction)%>% head(10)
#tabulate the target variable versus the predictions 
#Generating the confusion matrix
table(high_use=alc$high_use, prediction=alc$prediction)

#continue to explore the data
library(dplyr)
library(ggplot2)

#initialize a plot of 'high_use' versus 'probability' in alc
g<- ggplot(alc, aes(x = probability, y= high_use, col = prediction))
#define the geom as points and drw the plot
g+geom_point()
#tabulate the target variable vesus the predictions
table(high_use = alc$high_use, prediction = alc$prediction)%>% prop.table%>% addmargins
#what is the function of prop.table(), addmargins function, general use of table in confusion matrix in logistic regression



```

**Bonus**: Perform 10 fold cross validation on your model. Does your model have better test set performance (smaller prediction error using 10 fold cross validation) compared to the model introduced in DataCamp (which has baout 0.26 error). Could you find such a model?

### cross validation
This is a method of testing a predictive model on unseen data
split the data into training and testing data, 
whereby the training data is used to find the model 
while the test data is used to make prediction and evaluate the model performance

**Accuracy and error**
calculate the proportion of correctly classifed objects

one round of cross validation involves  

1. partitioning a sample of data into complementary subsets  
2. Performing the analysis on one subset (the training set, larger)  
3. validating the analysis on the other subset (the testing set, smaller)  

This process is repeated so that eventually all of the data is used for both
training and testing

In CV, the value of a penalty(loss) function (mean prediction error) is computed on data not used for finding the model. Low value= good.

Cross validation gives a good estmate of the actual predictive power of the model
It can also be sued to compare different models or classification methods.


```{r}
#define a lss function (average prediction error)
#define a loss function (mean prediction error)

loss_func<- function(class, prob){
  n_wrong<-abs(class - prob)> 0.5
  mean(n_wrong)
}
#call loss_func to compute the average number of wrong predictions in the training data
loss_func(class = alc$high_use, prob = alc$probability) #average number of wrong predictions

```


```{r}
#define a lss function (average prediction error)

loss_func<- function(class, prob){
  n_wrong<-abs(class - prob)>0.5
  mean(n_wrong)
}

#compute the average number of wrong predictions in the (training) data
loss_func(class= alc$high_use, prob = alc$probability)
#K fold cross validation
library(boot)
cv<- cv.glm(data= alc, cost= loss_func, glmfit= m, K=10)
#average number of wrong predictions in the cross validation
cv$delta[1]
```


**super bonus** Perform cross-validation to compare the performance of different logistic regression models (=different sets of predictors). Start with a very high number of predictors and explore the changes in the training and testing errors as you move to model with less predictors. draw a graph displaying the trends pf both training and testing errors by the number of predictors in the model

```{r}

```




