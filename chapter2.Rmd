# Regression and model validation

*Describe the work you have done this week and summarize your learning.*

- Describe your work and results clearly. 
- Assume the reader has an introductory course level understanding of writing and reading R code as well as statistical methods.
- Assume the reader has no previous knowledge of your data or the more advanced methods you are using.  

### Introduction to the data  
In this work space, a student feedback data(more information [here](www.mv.helsinki.fi/home/knehkala/JYTmooc/JYTOPKYS3-meta.txt)) collected between 2014-2015 was used to asses the relationship between learning approaches and the students achievement in an introductory statistics course. This dataset will be used to develop a linear regression model and interpret the results.  

### Load the data into your Rworkspace from your work directory
```{r}
#set the working directory of your R session the IODS project folder
setwd("Z:/Courses/Open data science/R/IODS-project")
#Read the file and ensure the header= TRUE and stringAsFactors=TRUE to ensure that the factor are not displayed as strings
students2014<-read.csv("Data/learning2014.csv", header = TRUE, stringsAsFactors = TRUE)
```

Explore the data structure and its dimensions

```{r}
str(students2014)
dim(students2014)


```
The data contains 166 observations of 7 variables. The observations refers to the students and the variables refer to student gender (male or female), age (derived the date of birth), exam points, attitude, deep, stra and surf.

```{r}



```


### Graphical overview and summaries of the variables in the data
```{r}
summary(students2014)

```
The mean age of the students in the class is 25 years, with female students being almost twice more than the male students in the course.

#plotting several barplots for visualization in scatter plot

#### Visualizing  student attitude aganist exam points
```{r}
library(ggplot2)

#initialize plot with data and aesthetic mapping
p1<- ggplot(students2014, aes( x= attitude, y= points, col = gender))
#define the visualization type (points)
p2<- p1 + geom_point()
#draw the plot
p2
#add a regression line
p3<- p2+geom_smooth(method = "lm")
#add a main title and draw the plot
p4<- p3 + ggtitle("student's attitude versus exam points")
p4
```

**Scatterplot matrix of the variables in the data**

```{r}
# exclude the gender variable from the matrix
pairs(students2014[-1], col = students2014$gender)

```


**Create advanced plot matrix with ggpairs()**
```{r}
#Access the GGally and ggplot2 libraries
library(GGally)
#create a more advanced plot matrix with ggpairs()
p<- ggpairs(students2014, mapping = aes (col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
#draw the plot
p

```




From the scatter plot, across both gender the attitude of the student is highly correlated to the exam points attained. While there is higher likelihood of male students failing in deep learning compared to female students


### Regression model
Linear regression model is a statistical approach that is used for modelling relationships between a dependent variable **y** and one or more explanatory variables **X**. If there is only one explanatory variable then its called simple linear regression while in more than one variable it is referred as multiple linear regression.  
Linear model is divided into two parts namely systematic and stochastic parts. The model follows the equation below
$$Y \sim \propto + X\beta_1+X\beta_2.....\beta_k +\epsilon $$  


  * where **Y** is the target variable, we wish to predict the values of **y**       using the values of **X** 
  * $\propto+X\beta_0+X\beta_1$ is the systematic part of the model 
  * $\beta$ quantifies the relationship between **y** and **x**.
  * $\epsilon$ describes the errors, which is assumed to be normally distributed.  
  
Using the student data set, the exam points has been used as the target variable while surface learning, strategic learning and attitude of the students have be used as independent variables in the model. The three predictors or independent variables were selected based on the correlation between them and the response variable.

```{r}
my_model<- lm(points~ attitude+stra+age, data = students2014)
summary(my_model)
```

The **call formula** captures the model fitted, e.g. in this case it is a multilinear regression with points as target variables while attitude, strategic learning and age are the independent variables.  

**Residuals**
Residuals are the difference between the actual observed response value (exam points) and the response values that the model predicted.  
The residual section is divided into 5 summary points, and it is used to assess how symmetrical distribution of the model across the points on the mean value zero. From the analysis, we can see the distribution of the residual appear abit symmetrical.  

**Coefficients**  
Coefficient gives unknown constants that represent the *intercept* and *unknown parameters* in terms of the model. Under the coefficient section , there are four variables.   

1. *Estimate*
The coefficient Estimate contains four rows ; the first one is the intercept $\propto$ value, while the rest correspond to estimates of the unknown parameters $\beta_1+\beta_2 ....\beta_k$ in our multi linear regression model.  

2. *Coefficient -Standard error*    
The coefficient Standard Error measures the average amount that the coefficient estimates vary from the actual average value of the response variable (exam points). Ideally, this value should be lower than its coefficient.  

3. *Coefficient-t value*
The coefficient t-vale is a measure of how many standard deviations our coefficient estimate is far from 0. When the t-value is far from zero, this would indicate that we can reject the null hypothesis, that is, we declare there relationship between the dependent variables and the target variable exist.In our analysis, the t-statistic value of *attitude* is relatively far away from zero and is larger relative to the standard error, which would indicate a relationship exist, compared to variables *stra* and *age*.  

4. *Coefficient -Pr(<t)*
Commonly referred to as the p-value, a small p-vale indicates that it is unlikely to observe a relationship between the predictors and the target variable due to chance. Typically, a p-value of 5% or less is a good cut -off point. In our model, the p-vales for *intercept* and *attitude* are more closer to zero compared to *stra* and *age*.  

The "signif.Codes" are associated to each estimate, a three star (or asterisks) represent a highly significant p-value. The three predictors were significant (p $\leqslant$ 0.05), meaning that we can reject the null hypothesis allowing us to conclude that there is a relationship between *exam points* and three student variables *attitude, stra and age*.  

**Residual standard error** is a measure of the quality of a linear regression fit. The residual standard error is the average amount that the response (exam points) will deviate from the true regression line.In _my_model_ the response _exam points_ deviated from the true regression line by approximately 5.26 on average. Note that the Residual Standard Error was calculated with 162 degrees of freedom (DOF). DOF are the number of data points that went into the estimation of the parameters used after taking into account these parameters. In this analysis, we had 166 observation (students), intercept and three predictors parameters.    

**Multiple R-squared**  
The R-squared also known as coefficient of determination ($R^2$) statistics provides the percentage of the variation within the dependent/response/target variable that the independent variables are explaining.In other word, it describes how well the model is fitting the data. $R^2$ always lies between 0 and 1, with values approaching 0 indicating a regression that does not explain the variance in the response variable well, while a value closer to 1 explains the variance in the response variable).In _my_model_, the $R^2$ is 21.82%, meaning that only 22% of the variance found in the response variable (exam point) can be explained by the predictors _attitude, stra and age_. 
Adjusted R-squared is shows what percentage of the variation within the dependent variable that all predictors  are explaining.




model assumptions
an obvious assumption is linearity.
the target variable is modeled as a linear combination of the model parameters
assumed that the errors are normally distributed



### Diagnostic plots
Three diagnostic plots were plotted from the model  

1. Residual Vs Fitted  
  This plot explores the validity of the model assumptions that are included in   the expression
  $$
  \epsilon \sim N(0,\sigma^2)
  $$
  * The errors are normally distributed
  * The errors are not correlated
  * The errors have constant variance $\sigma^2$  
  * The size of a given error does not depend on the explanatory variables  
  
2. Normal Q-Q plot  
   Explores the assumption that the errors of the model are normally distributed.When the majority of the residual are falling along the line, then it validates the assumption of  normality. Severe deviation of the residuals from the normal line makes the assumption of normality questionable.
   
3. Residual vs Leverage plot  
   Leverage measures how much impact or influence a single observation has on the model. Not all outliers are influential in regression analysis.Cases which have high influence are usually located at the upper right corner or at the lower right corner. Presence of cases is such spots can be influential against a regression line.When cases are outside the Cook's distance, meaning they have high Cook's distance scores) , the cases are influential to the regression results, and the regression results will be altered if they are excluded.

```{r}
par(mfrow= c(2,2))
plot(my_model, which = c(1,2,5))

```

In the **Residual vs Fitted** plot, the residual are distributed without following a distinctive pattern indicating that the linear relation ship was explained by the model. 

In the **Normal Q-Q plot**, most of the residual points are falling along the diagonal line, with little deviation at both ends.Since there is minimal deviation of the residuals from the line, the QQ plot strengthens the assumption of normality of the model

In the **leverage plot** there are no influential cases as we can see the Cook's distance as all the cases are well inside of the Cook's distnace lines.


```{r}

date()
```

References  
1. [Rmarkdown for Scientist](https://rmd4sci.njtierney.com)  
2. [University of Virginia Library](https://data.library.virginia.edu/diagnostic-plots/)




