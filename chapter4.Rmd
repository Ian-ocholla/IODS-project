# CLUSTERING
This exercises utilize Boston data found in MASS package.  
The data is about housing in surburban area in Boston


```{r}
library(MASS)
library(corrplot)
library(tidyr)
library(ggplot2)

```



### 1. Data exploration
```{r}
str(Boston)

```

The Boston data set has 506 observations and 14 variables. The observation values were captured either as integers or numeric data types

#### 1.1 Summary statistics
```{r}
summary(Boston)

```

The summary shows the descriptive statistics of each of the variable in the Boston data. The variable *tax* had the largest range between the minimum and maximum values while *chas* had the least range

#### 1.2 Correlation analysis.  
##### 1.2.1 plots between the variables.  
```{r}
#plot matrix of the variables
pairs(Boston[1:6])
pairs(Boston[7:14])

```

##### 1.2.2 Correlation matrix 
```{r}
#between variables in the data
cor_matrix<- cor(Boston)%>% round(digits = 2)
#print the correlation matrix
cor_matrix


```

##### 1.2.3 Visualization of the correlation matrix
```{r}
corrplot(cor_matrix, method="circle",type= "upper",
         cl.pos="b", tl.pos="d",tl.cex=0.6)

```

From the correlation plots and tables, the variables _tax_ and _rad_ are highly positive correlated (corr> 0.8). This is further captured in the visualization of the correlation represented by the bigger circle. _Medv_ and _lstat_, _dis_ and _age_, and _dist_ and _nox_ have strong negative correlation but not significant.  

### 2.0 Data analysis.    

#### 2.1 Standardizing the data set 
Standardizing is done through scaling. This entails subtracting the column means from the corresponding columns and divide the difference with standard deviation

```{r}
#* center and standardize variables
boston_scaled<- scale(Boston)
#summaries of the scaled variables
summary(boston_scaled)

```

* All the data have been standardized with **all the variables** having a **mean of zero (0)**. Compared to the original data where the data ranged between 0.00 to 711 with varying mean values.  

#### 2.2 Creation of a new a categorical variable
Create of a new a categorical variable of the crime rate in the Boston data set from the scaled crime rate

```{r}
#class of the boston_scaled object
class(boston_scaled)
#change the object to data frame
boston_scaled<- as.data.frame(boston_scaled)
#summary of the scaled crime rate
summary(boston_scaled$crim)

```
The range of the crime rate is between -0.419 (min) to 9.924 (max), with a mean of 0.


#### 2.3 Create break points to be used in the categorical variable
```{r}
#create a quantile vector of crim and print it
bins<- quantile(boston_scaled$crim)
bins

```

Divides the data into four quantiles.  

#### 2.4 Creating break points in the categorical variable

```{r}
crime<- cut(boston_scaled$crim, breaks= bins, include.lowest= TRUE, labels=c("low","med_low","med_high","high"))

#look at the table of the new factor crime
table(crime)

```
Low and high crime rate had 127 observation, while med_low and med_high had 126

#### 2.5 Remove existing variable and adding new variable in the dataset
```{r}
#remove the old crime variable from dataset
boston_scaled<- dplyr::select(boston_scaled, -crim)
#add the new categorical value to scaled data
boston_scaled<- data.frame(boston_scaled, crime)
```

#### 2.6 Creation of training and test data sets from the dataframe.  

Divide the dataset to train (80%) and test sets (20%). Create the train and test variables and removing the original crime variable.  

```{r}
#number of rows in the Boston dataset
n<-nrow(boston_scaled)
#choose randomly 80% of the rows
ind<- sample(n, size=n * 0.8)
#create train set
train<- boston_scaled[ind,]
#create test set
test<- boston_scaled[-ind,]
#save the correct classes from test data
correct_classes<- test$crime
#remove the crime variable from test data
test<- dplyr::select(test, -crime)

```

### 3.0 Linear Discriminant Analysis 

Linear Discriminant Analysis is a classification ( and dimension reduction) method. It finds the (linear) combination of the variable that separate the target variable classes use the categorical crime rate as the target variable and all other variables in the data set as predictor variable

```{r}
#*Fit the linear discriminant analysis on the train set.
lda.fit<- lda(crime~., data= train)
# print the lda.fit object
lda.fit
```
LDA determines group means and computes , for each individual, the probability of belonging to the different groups of the target variable. The individual observation is then affected to the group with the highest probability score. 

**Interpreting the results**  
 _**Prior probabilities of groups**_ : The proportion of training observation in each group.Such that:  
 
* There are 23.5% of training observations in the low 
* 25% of training in the med_low category
* 26.48% of training observation in med_high category
* 25% of training observation in high category.  

_**Group means**_: represent the group center of gravity. It shows the mean of each variable in each group.  
_**Coefficient of linear discriminant**_: Shows the linear combination of predictor variables that are used to form the LDA decision rule.  
_**Proportion of trace**_: shows the variability of each linear dimension, with LD1 having the highest with 94.11 while LD3 with the least at 1.62%

#### 3.1  Draw the LDA (bi)plot

LDA Biplot is designed to show how individual and groups are different.  

```{r}
#create function for lda biplot arrows
lda.arrows<- function(x, myscale=1, arrow_heads=0.1, color="orange",
                      tex= 0.75, choices=c(1,2)){
  heads<- coef(x)
  arrows(x0 =0, y0=0,
         x1= myscale * heads[,choices[1]],
         y1= myscale* heads[,choices[2]], col = color, length=
           arrow_heads)
  text(myscale*heads[,choices], labels=row.names(heads),
                     cex= tex, col=color, pos=3)
}
#target classes as numeric 
classes<- as.numeric(train$crime)
#plot the lda results
plot(lda.fit, dimen=2, col=classes, pch= classes)
lda.arrows(lda.fit, myscale=1)

```

In LDA biplots the ""arrows" represent the variables. The longer arrows represent more discrimination. In the above plot, _**rad**_ variable has more variation (larger standard deviation) compared to other variables. This means it is the most influential variable in the model.  

In addition, the variables _**nox**_ and _**rad**_ are not highly correlated as the angle between them is nearly right angle.Variables _**nox**_ and _**zn**_ are negatively correlated.    

#### 3.2 Predict a class with the LDA model on the test data
```{r}
lda.pred<- predict(lda.fit, newdata=test)
#*  cross tabulate the results with the crime categories from the test set.
table(correct= correct_classes, predicted= lda.pred$class)

```

The predicted low and med_high categories had higher proportion of correct cluster observations compared to the test clusters, whereas the test clusters had higher proportion of correct observation in high and med_low categories.   

### 4.0 K-Means clustering

Reload the Boston dataset and standardize
```{r}
library(MASS)
data("Boston")
#center and standardize variables
Boston_scaled<- scale(Boston)
#summaries of the scaled variables
summary(Boston_scaled)
str(boston_scaled)

```
From the summary, all the variables have a ***mean value of zero*** after being standardized

#### 4.1 Calculate the distances between the observations.  
##### 4.1.1Euclidean distance matrix
```{r}
#euclidean distance matrix
dist_eu<- dist(Boston_scaled)
#summary of the distances
summary(dist_eu)

```

##### 4.1.2 Manhattan distance matrix
```{r}
#euclidean distance matrix
dist_man<- dist(Boston_scaled, method = 'manhattan')
#summary of the distances
summary(dist_man)

```

#### 4.2 k-means analysis 

Implement K-means analysis with a random number of K-means
```{r}
#K-means clustering
km<- kmeans(Boston_scaled, centers = 3)
#Plot the Boston_scaled dataset with clusters
pairs(Boston_scaled, col= km$cluster)

```

All the pairs have been clustered into three clusters- green, black and brown.  

#### 4.3 Determine the optimal number of k means

To determine the optimal number of Kmeans we look at how the total within cluster sum of squares (WCSS) behaves when the number of cluster changes.When you plot the number pf clusters and the total WCSS, the optimal number of clusters is when the total WCSS drops radically

```{r}
#set set.seed function to avoid the random assigning of initial clusters centers
set.seed(123) #to avoid random
#determine the number of clusters
k_max<-10
#calculate the total within sum of squares
twcss<- sapply(1:k_max, function(k)
  {kmeans(Boston_scaled, k)$tot.withinss})
#visualize the results
qplot(x = 1:k_max, y= twcss, geom= 'line')

```

From the plot the point at which there was instant change is probably at 2.  
This will act as the optimal number of K-means.  

Implement the Kmeans with the optimal number of centers = 2

```{r}
#k-means clustering
km<- kmeans(Boston_scaled, centers= 2)
#plot the Boston dataset with clusters, vary the number of pairs
pairs(Boston_scaled,col = km$cluster)


```

From the two plot, the two clusters are distinct from each other. This can be attributed to K-means ability to enhance separability of different clusters.  

**Bonus**: 

* Perform k-means on the original Boston data with some reasonable number of clusters (>2). Remember to standardize the dataset.  
* Then perform LDA using the clusters as target classes. Include all the variables in the boston data in the LDA model.  
* Visualize the results with a biplot (incode arrows representing the relationship of the original variables to the LDA solution).  
* Interpret the results. Which variables are the most influencial linear separators for the clusters?

##### 1a. Load and standardize the original dataset.  
```{r}
#Reload the Boston datset and standardize the dataset
library(MASS)
data("Boston")

#center and standardize variables
boston_scaled2<- scale(Boston)
#summaries of the scaled variables
summary(boston_scaled2)

```

All the variables have been assigned to a mean value of **zero**.  

##### 1b. Perform K-means using some reasonable number of clusters (>2).

```{r}
#k-means clustering
km<- kmeans(boston_scaled2, centers= 5)
#plot the Boston dataset with clusters, vary the number of pairs
pairs(boston_scaled2,col = km$cluster)

```

All the pairs have 5 clusters

##### 1c Add the clusters into the scaled boston dataset

```{r}
#add the clusters into the boston dataset
boston_scaled2<- data.frame(boston_scaled2, km$cluster)

```

##### 1d. Create training and test dataset from the scaled boston data

```{r}
#number of rows in the Boston dataset
n<-nrow(boston_scaled2)
#choose randomly 80% of the rows
ind<- sample(n, size=n * 0.8)
#create train set
train2<- boston_scaled2[ind,]
#create test set
test2<- boston_scaled2[-ind,]
```

The original data was divided into 80% training and 20% testing sets.  

##### 1e.Perform LDA using the clusters as target classes. Include all the variables in the boston data in the LDA model.

```{r}
#*Fit the linear discriminant analysis on the train set.
lda.fit2<- lda(km.cluster~., data= train2)
# print the lda.fit object
lda.fit2

```


**Interpreting the results**  
 _**Prior probabilities of groups**_ : The proportion of training observation in each group.Such that:  
 
* There are 7.42% of training observations in cluster 1
* 13.61% of training observation in cluster 2  
* 23.26% of training observation in cluster 3  
* 20.79% of training observation in cluster 4  
* 34.90% of training observation in cluster 5

_**Group means**_: represent the group center of gravity. It shows the mean of each variable in each group.  
_**Coefficient of linear discriminant**_: Shows the linear combination of predictor variables that are used to form the LDA decision rule.  
_**Proportion of trace**_: shows the variability of each linear dimension as follows:  

* LD1 having the highest of the variability with 63.88%  
* LD2 having 25.88%  
* LD3 8.18%  
* LD4 2.06%




Draw the LDA (bi)plot

```{r}

#create function for lda biplot arrows
lda.arrows<- function(x, myscale=1, arrow_heads=0.1, color="orange",
                      tex= 0.75, choices=c(1,2)){
  heads<- coef(x)
  arrows(x0 =0, y0=0,
         x1= myscale * heads[,choices[1]],
         y1= myscale* heads[,choices[2]], col = color, length=
           arrow_heads)
  text(myscale*heads[,choices], labels=row.names(heads),
       cex= tex, col=color, pos=3)
}
#target classes as numeric 
classes<- as.numeric(train2$km.cluster)
#plot the lda results
plot(lda.fit2, dimen=2, col=classes, pch= classes)
lda.arrows(lda.fit2, myscale=1)

```

From the plot, , _**char**_ variable has more variation (larger standard deviation) and is the most influential variable compared to other variables as represented by the longest arrow, this is followed by variable _**rad**_. Moreover, the two variable _**chas**_ and _**rad**_ are not highly correlated as the angle between them is nearly a right angle   


#### **Super-Bonus**
```{r}
model_predictors<- dplyr::select(train,-crime)
#check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)
#matrix multiplication
matrix_product<- as.matrix(model_predictors)%*%
  lda.fit$scaling
matrix_product<-as.data.frame(matrix_product)

```

Access plotly package to create a 3D plot of the columns of the matrix product.  

* Adjust the code: add argument color as an argument in the plot_ly8) function.  
* set the color to be the crime classes of the train set.  
* Draw another 3 D plot where the color is defined by the clusters of the K-means.  
* How do the plots differ? Are there any similarities?
```{r}
library(plotly)
plot_ly(x= matrix_product$LD1, y= matrix_product$LD2, 
        z= matrix_product$LD3, type='scatter3d', mode= 'markers')

```



```{r}

date()
```

```{r}


```

```{r}


```

